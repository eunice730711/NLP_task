{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10 #(0~9)\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義embedding matrix，亂數初始化，利用end-to-end training來學習vector representation\n",
    "(之後可以直接套用word2vec???)\n",
    "\n",
    "矩陣為大小為： 單字數量 * 單字embedding(word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用tf.nn.embedding_lookup直接查閱embedding matrix，比方說encoder_inputs為[0,0,1,0,0,1,0,0,0,0](使用one-hot encoding，分別代表0~9的單字)\n",
    "則encoder_inputs_embedded回傳word 2和word 5的word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "_, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs_embedded,\n",
    "    dtype=tf.float32, time_major=True,\n",
    ")\n",
    "# del encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此我們只在乎encoder_final_state(LSTM的最後一個time step的hidden layer)\n",
    "\n",
    "關於dynamic_rnn吃的參數：\n",
    "dynamic_rnn(\n",
    "    cell,\n",
    "    inputs,\n",
    "    sequence_length=None,\n",
    "    initial_state=None,\n",
    "    dtype=None,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    time_major=False,\n",
    "    scope=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow中的LSTM利用Tuple of Tensor型態來儲存state\n",
    "<li>encoder_final_state.h is activations of hidden layer of LSTM cell</li>\n",
    "<li>encoder_final_state.c is final output, which can potentially be transfromed with some wrapper @TODO: check correctness</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell, decoder_inputs_embedded,\n",
    "    initial_state=encoder_final_state,\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需將encoder_final_state傳入到decoder的initial_state，因此他們必須是compatible(相同的cell：encoder用LSTM則decoder也用LSTM、相同的hidden units和layer個數）-> 或許可以加上one-layer MLP在final_state以relax此限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>decoder_cell output is a hidden_units sized vector at every timestep，為了將decoder_ouputs的size從原本的hidden_units size轉成vocab_size，我們放了一個線性layer作轉換</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fully_connected/Reshape_1:0' shape=(?, ?, 10) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>RNN輸出為tensor，其shape為[max_time, batch_size, hidden_units]而後利用一個projection layer來mappint到shape為[max_time, batch_size, vocab_size]</li>\n",
    "<li>vocab_size為static, max_time和batch_size是dynamic</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax:0' shape=(?, ?) dtype=int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "關於tf.nn.softmax_cross_entropy_with_logits：\n",
    "<li>參數logits：就是神經網絡最後一層的輸出，如果有batch的話，它的大小就是[batchsize, num_classes]，單樣本的話，大小就是num_classes</li>\n",
    "<li>參數labels：實際的標簽，大小同上</li>\n",
    "<li>注意：此函數的回傳值為一個向量，若要求loss的值須額外多做一步tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test forward pass\n",
    "先隨便假設一組資料試試看各個dimension是否符合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[6 3 9]\n",
      " [0 4 8]\n",
      " [0 0 7]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "decoder predictions:\n",
      "[[2 2 4]\n",
      " [7 7 4]\n",
      " [7 7 4]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "batch_ = [[6], [3, 4], [9, 8, 7]]\n",
    "\n",
    "batch_, batch_length_ = helpers.batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_, dlen_ = helpers.batch(np.ones(shape=(3, 1), dtype=np.int32),\n",
    "                            max_sequence_length=4)\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the toy task\n",
    "接著我們random出簡單sample來做測試，我們現在以0~9分別代表9個單字作為範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[8, 3, 9, 2, 5, 3, 6]\n",
      "[7, 4, 4, 5, 5, 5]\n",
      "[6, 7, 4, 8, 8, 8, 9]\n",
      "[3, 2, 4, 8]\n",
      "[4, 6, 4, 6, 8, 3]\n",
      "[4, 6, 9, 8]\n",
      "[8, 2, 6, 6]\n",
      "[6, 7, 8, 4, 3, 9]\n",
      "[7, 3, 9, 2, 7, 8, 8]\n",
      "[7, 3, 7, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, _ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, _ = helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        decoder_inputs: decoder_inputs_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given encoder_inputs [5, 6, 7], decoder_targets would be [5, 6, 7, 1], where 1 is for EOS, and decoder_inputs would be [1, 5, 6, 7] - decoder_inputs are lagged by 1 step, passing previous token as input at current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.26916456223\n",
      "  sample 1:\n",
      "    input     > [7 4 6 2 4 0 0 0]\n",
      "    predicted > [2 5 5 5 5 9 6 6 6]\n",
      "  sample 2:\n",
      "    input     > [7 3 5 6 4 0 0 0]\n",
      "    predicted > [2 5 8 3 0 0 0 6 6]\n",
      "  sample 3:\n",
      "    input     > [3 4 5 8 2 9 0 0]\n",
      "    predicted > [4 8 8 8 8 4 5 4 4]\n",
      "()\n",
      "batch 1000\n",
      "  minibatch loss: 0.347310572863\n",
      "  sample 1:\n",
      "    input     > [3 7 8 4 0 0 0 0]\n",
      "    predicted > [3 7 8 4 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 7 9 2 0 0 0]\n",
      "    predicted > [7 7 7 9 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 3 7 7 9 4 3 0]\n",
      "    predicted > [8 3 7 7 9 4 3 1 0]\n",
      "()\n",
      "batch 2000\n",
      "  minibatch loss: 0.183116927743\n",
      "  sample 1:\n",
      "    input     > [3 9 7 0 0 0 0 0]\n",
      "    predicted > [3 9 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 4 9 4 2 2 8 8]\n",
      "    predicted > [3 4 9 4 2 8 8 1 1]\n",
      "  sample 3:\n",
      "    input     > [4 4 7 2 4 2 0 0]\n",
      "    predicted > [4 4 7 2 4 2 1 0 0]\n",
      "()\n",
      "batch 3000\n",
      "  minibatch loss: 0.107933960855\n",
      "  sample 1:\n",
      "    input     > [2 3 8 0 0 0 0 0]\n",
      "    predicted > [2 3 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 2 4 3 9 4 9 8]\n",
      "    predicted > [5 2 4 3 9 4 9 8 1]\n",
      "  sample 3:\n",
      "    input     > [4 5 5 5 0 0 0 0]\n",
      "    predicted > [5 5 5 5 1 0 0 0 0]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1152 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhRTuIiJR6P8Bd9H6Xdd1iFIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118365410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考資料：\n",
    "<li>https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/1-seq2seq.ipynb</li>\n",
    "<li>http://blog.csdn.net/mao_xiao_feng/article/details/53382790</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
